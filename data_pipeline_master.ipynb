{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "path_311 = \"data/311_Service_Requests_from_2010_to_Present.csv\"\n",
    "path_nta = \"data/Neighborhood Tabulation Areas.geojson\"\n",
    "path_nta_population = \"data/New_York_City_Population_By_Neighborhood_Tabulation_Areas.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311 = pd.read_csv(path_311)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that \"Unique Key\" column can be an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"Unique Key\"].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311.set_index(\"Unique Key\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check what complaint types do not have \"Location\" set. If there would be few of those or the categories would be of no interest to us, we would have dropped them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[df_311[\"Location\"].isna()].groupby(\"Complaint Type\").count()[\"Unique Key\"].sort_values()[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the State Plane coordinates as they are just a different format of the coordinates present in Latitude and Longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311.drop(columns=[\"X Coordinate (State Plane)\", \"Y Coordinate (State Plane)\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the \"Location\" column as it is just the Latitude and Longitude columns combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311.drop(columns=\"Location\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After dropping Location and X/Y Coordinates in State Plane "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the complaint types of the entries with \"Bridge Highway Segment\" present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_segment_present = df_311[~df_311[\"Bridge Highway Segment\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_segment_present[\"Complaint Type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"Complaint Type\"].value_counts(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We realize that some of the complaint types are incorrct. Many of such entries categories appear only once in the dataset. We drop those entries as we cannot interpret such complaints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_complaints = list(df_311[\"Complaint Type\"].value_counts(ascending=True)[\n",
    "    df_311[\"Complaint Type\"].value_counts(ascending=True) == 1].index)\n",
    "df_311 = df_311[~df_311[\"Complaint Type\"].isin(invalid_complaints)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casting fields to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_created_dates = df_311[\"Created Date\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_closed_dates = df_311[\"Closed Date\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_closed_dates = list(pd.DataFrame(unique_closed_dates)[0].dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will verify if the fields with dates have proper formatting. This still does not guarantee that they are logically correct, that is in some expected range, but tells us whether or not we would be able to parse them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt='01/17/2026 10:12:35 PM'\n",
    "\n",
    "# MMDDYYYY 1\n",
    "re1='((?:[0]?[1-9]|[1][012])[-:\\\\/.](?:(?:[0-2]?\\\\d{1})|(?:[3][01]{1}))[-:\\\\/.](?:(?:[1]{1}\\\\d{1}\\\\d{1}\\\\d{1})|(?:[2]{1}\\\\d{3})))(?![\\\\d])'\n",
    "# White Space 1\n",
    "re2='( )'\t\n",
    "# HourMinuteSec\n",
    "re3='((?:(?:[0-1][0-9])|(?:[2][0-3])|(?:[0-9])):(?:[0-5][0-9])(?::[0-5][0-9])?(?:\\\\s?(?:am|AM|pm|PM))?)'\n",
    "\n",
    "rg = re.compile(re1+re2+re3,re.IGNORECASE|re.DOTALL)\n",
    "\n",
    "def try_matching(txt):\n",
    "    m = rg.search(str(txt))\n",
    "    if not m:\n",
    "        print(\"NOT A MATCH!\")\n",
    "        print(txt)\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_created = list(map(lambda x: try_matching(x), unique_created_dates))\n",
    "sum(mapped_created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_closed = list(map(lambda x: try_matching(x), unique_closed_dates))\n",
    "sum(mapped_closed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one entry among closed dates that is in the third millenium. We assume time travel is impossible, so the complaint couldn't have been closed in the future. Dropping the entry with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311 = df_311[df_311[\"Closed Date\"] != \"03/30/3027 12:00:00 AM\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing the dates and casting them to pandas datetime type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"Created Date\"] = pd.to_datetime(df_311[\"Created Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"Closed Date\"] = pd.to_datetime(df_311[\"Closed Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"Created Date\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the \"Created Date\" column appears to contain only dates that are possible (within reasonable range. Moreover, we clearly see an increase in the number of complaints overall since the year 2012. This might be either due to growing popularity of 311, increase in population or worsening conditions in the city. It might be a combination of those factors, so we do not draw final conclusions yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"Closed Date\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly there is something wrong with the values of ticket closed date. There are entries that have the dates set in 1900s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[df_311[\"Closed Date\"] < datetime.datetime(2009, 1, 1)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that those which do not have \"Status\" set to closed should not have the \"Closed Date\" set in the first place as the only other status present among them is \"Pending\". Thus we set their \"Closed Date\" to NaN. We remove the entries which had the status closed and \"Closed date\" set before 2010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[(df_311[\"Closed Date\"] < datetime.datetime(2010, 1, 1)) & (df_311[\"Status\"] == \"Closed\")][\"Unique Key\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311.drop(df_311[(df_311[\"Closed Date\"] < datetime.datetime(2010, 1, 1)) & (df_311[\"Status\"] == \"Closed\")].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the issues we have spotted is the fact that there are entries with \"Closed Date\" before \"Created Date\". We assume this might be a way of dealing with complaints submitted for the problems that were already resolved. Those may be also plain mistakes. We decide to drop all such rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311.drop(df_311[df_311[\"Closed Date\"] < df_311[\"Created Date\"]].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"Closed Date\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also decide to remove the rows that have \"Closed Date\" after today. This is because one expects the tickets with \"Closed Date\" present to be actually closed already. There is few such cases, so it should not pose a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[df_311[\"Closed Date\"] > datetime.datetime.today()][\"Unique Key\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311.drop(df_311[df_311[\"Closed Date\"] > datetime.datetime.today()].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two more columns with dates in them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"Due Date\"] = pd.to_datetime(df_311[\"Due Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"Due Date\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that the Due Dates set in 1900s are incorrect and replace them by NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[df_311[\"Due Date\"] < datetime.datetime(2010, 1, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"Due Date\"] = df_311[\"Due Date\"].replace(datetime.datetime(1900, 1, 2), np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"Due Date\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_resolution_dates = df_311[\"Resolution Action Updated Date\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(map(lambda x: try_matching(x), unique_resolution_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_without_incorrect = df_311[~df_311[\"Resolution Action Updated Date\"].isin(incorrect_dates)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_without_incorrect[\"Resolution Action Updated Date\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_dates = [\"10/30/2926 11:51:00 AM\", \"05/25/2510 10:15:00 AM\", \"03/06/2927 12:30:00 PM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"Resolution Action Updated Date\"] = df_311[~df_311[\"Resolution Action Updated Date\"].isin(incorrect_dates)][\"Resolution Action Updated Date\"]\n",
    "df_311[\"Resolution Action Updated Date\"] = pd.to_datetime(df_311[\"Resolution Action Updated Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"Resolution Action Updated Date\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the rows where the Resolution Action Updated Date is in the future. By definition it should be the date when agency has last updated the entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311.drop(df_311[df_311[\"Resolution Action Updated Date\"] > datetime.datetime.today()].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"Resolution Action Updated Date\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to drop the entries with \"Resolution Action Updated Date\" before 2010 as that is the date where the data has started being collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311.drop(df_311[df_311[\"Resolution Action Updated Date\"] < datetime.datetime(2010, 1, 1)].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"Resolution Action Updated Date\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casting to \"category\" type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that many of the columns can take only one of a small set of possible values. We cast them to \"category\" type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_columns = [\"Agency\", \"Agency Name\", \"Complaint Type\", \"Descriptor\", \"Location Type\",\n",
    "                    \"Community Board\", \"Address Type\", \"City\", \"Landmark\", \"Facility Type\", \"Status\",\n",
    "                    \"Resolution Description\", \"Borough\", \"Open Data Channel Type\", \"Park Facility Name\",\n",
    "                    \"Park Borough\", \"Vehicle Type\", \"Taxi Company Borough\", \"Bridge Highway Direction\", \"Road Ramp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[category_columns] = df_311[category_columns].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311.to_pickle(\"data/311_Service_Requests_from_2010_to_Present_small.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df_311 = pd.read_pickle(\"data/311_Service_Requests_from_2010_to_Present_small.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate wordcloud from Complaint Type, Descriptor, and Resolution Descriptor. Those can be narrowed down later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def create_wordcloud(pandas_series, max_words=2000, max_font_size=None, mask=None, word_level=False, filepath=None):\n",
    "    wordcloud = WordCloud(\n",
    "        width=3000,\n",
    "        height=2000,\n",
    "        max_words=max_words,\n",
    "        max_font_size=max_font_size,\n",
    "        background_color='white',\n",
    "        stopwords=STOPWORDS,\n",
    "        random_state=1,\n",
    "        mask=mask,\n",
    "        contour_width=3,\n",
    "        contour_color='black'\n",
    "    )\n",
    "    \n",
    "    text = pandas_series.astype(str).values\n",
    "    processed_text = wordcloud.process_text(\" \".join(text))\n",
    "    wordcloud.generate_from_frequencies(processed_text)\n",
    "    fig = plt.figure(\n",
    "        figsize = (40, 30),\n",
    "        facecolor = 'k',\n",
    "        edgecolor = 'k')\n",
    "    plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout(pad=0)\n",
    "    if filepath:\n",
    "        wordcloud.to_file(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_mask = np.array(Image.open(\"data/NYC_silhouette.png\"))\n",
    "nyc_mask[nyc_mask > 0] = 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_wordcloud(df_311[\"Complaint Type\"], mask=nyc_mask, filepath=\"data/complaint_type_wc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_wordcloud(df_311[\"Resolution Description\"], filepath=\"data/resolution_description_wc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_wordcloud(df_311[\"Descriptor\"], filepath=\"data/resolution_descriptor_wc.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating how agencies are split to take care of smaller regions in NYC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df_311 = pd.read_pickle(\"data/311_Service_Requests_from_2010_to_Present_small.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school_agencies = [agency for agency in list(df_311[\"Agency Name\"].unique()) if \"School - \" in agency]\n",
    "len(school_agencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[df_311[\"Agency Name\"].isin(school_agencies)][\"Agency\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the agencies are schools. All of them are mapped to agency \"DOE\". Schools are divided into NYC school districts. The map of such districts is available https://data.cityofnewyork.us/Education/School-Districts/r8nu-ymqj , but we don't see a dataset that could map 3-1-1 Agency Names to school districts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_school_agencies = [agency for agency in list(df_311[\"Agency Name\"].unique()) if not \"School - \" in agency]\n",
    "len(not_school_agencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[df_311[\"Agency\"] == \"NYPD\"][\"Agency Name\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that a single \"Agency\" entry maps to multiple \"Agency Name\" entries that in some cases are equivallent, such as \"NYPD\" and \"New York City Police Department\". This indicates that we should first look at the \"Agencies\" themselves as \"Agency Names\" may not be meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_311[\"Agency\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NYPD is split into Police Precints: https://data.cityofnewyork.us/Public-Safety/Police-Precincts/78dh-3ptz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_311[df_311[\"Agency\"] == \"OMB\"][\"Agency Name\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DSNY (Department of Sanitation) districts: https://data.cityofnewyork.us/City-Government/DSNY-Districts/6j86-5s7z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FDNY (Fire Division). Battallions: https://data.cityofnewyork.us/Public-Safety/Fire-Battalions/uh7r-6nya , divisions (coarse grained): https://data.cityofnewyork.us/Public-Safety/Fire-Divisions/hkpx-aaxc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, we have found geographical subdivisions for DOE, NYPD, DSNY and FDNY."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Louis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_PATH = \"data/extract.csv\"\n",
    "path_nta = \"data/Neighborhood Tabulation Areas.geojson\"\n",
    "path_nta_population = \"data/New_York_City_Population_By_Neighborhood_Tabulation_Areas.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(SAMPLE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check that the Unique Key truly is unique, and that no NaN's appear in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df[\"Unique Key\"].is_unique\n",
    "assert not df[\"Unique Key\"].isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df.columns:\n",
    "    if \"Date\" in c:\n",
    "        print(\"Converting\", c)\n",
    "        df[c] = pd.to_datetime(df[\"Created Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Created Day\"] = df[\"Created Date\"].apply(lambda d: d.day)\n",
    "df[\"Created Month\"] = df[\"Created Date\"].apply(lambda d: d.month)\n",
    "df[\"Created Year\"] = df[\"Created Date\"].apply(lambda d: d.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatbarplot(value_counts):\n",
    "    max_v = value_counts.max()\n",
    "    min_v = value_counts.min()\n",
    "    clrs = {k:np.array([(v-min_v)/(max_v-min_v), 0., 0.]) for k, v in zip(value_counts.keys(), value_counts.values)}\n",
    "    sns.barplot(x=value_counts.keys(), y=value_counts.values, palette=clrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatbarplot(df[\"Created Month\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatbarplot(df[\"Created Year\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatbarplot(df[\"Created Day\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Agency\"].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ct_vcs = df[\"Complaint Type\"].value_counts()\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "ct_vcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ct_vcs.head().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotpts(geodf, color=\"#ff0000\", popup=\"None\"):\n",
    "    clean = geodf[geodf[\"Latitude\"].notnull()]\n",
    "    clean = clean[clean[\"Longitude\"].notnull()]\n",
    "    m = folium.Map(location=[clean[\"Latitude\"].values[0], clean[\"Longitude\"].values[0]], zoom_start=10)\n",
    "    #for c in zip(clean[\"Latitude\"].values, clean[\"Longitude\"].values):\n",
    "    for i, row in clean.iterrows():\n",
    "        folium.CircleMarker(location=(row[\"Latitude\"], row[\"Longitude\"]),\n",
    "                            radius= 1,\n",
    "                            color=color(row) if callable(color) else color,\n",
    "                            popup=popup(row) if callable(popup) else popup,\n",
    "                            fill=True).add_to(m)\n",
    "    return m\n",
    "plotpts(df[df[\"Complaint Type\"] == \"Noise - Residential\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfun = lambda row: \"#aa3300\" if row[\"Complaint Type\"] == \"Noise - Residential\" else \\\n",
    "\"#00aa33\" if row[\"Complaint Type\"] == \"Street Condition\" else \\\n",
    "\"#aa0066\" if row[\"Complaint Type\"] == \"Street Light Condition\" else \\\n",
    "\"#cccc00\" if row[\"Complaint Type\"] == \"HEAT/HOT WATER\" else \\\n",
    "\"#3300aa\" if row[\"Complaint Type\"] == \"Illegal Parking\" else \"#444444\"\n",
    "popup = lambda row: str(popup)\n",
    "df2k = df.sample(n=2000, random_state=1)\n",
    "plotpts(df2k, color=cfun, popup=popup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Out of\", df.shape[0], \"entries:\")\n",
    "facility_df = df[df[\"Facility Type\"].notnull()]\n",
    "print(facility_df.shape[0], \"have associated facility type.\")\n",
    "facility_df[\"Facility Type\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3 disctinct associated facility type. Mostly Precinct, then DSNY Garage then a few School."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Open Data Channel Type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Status\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Borough\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2k = df.sample(n=2000, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfun = lambda row: \"#00cc00\" if row[\"Status\"] == \"Closed\" else \\\n",
    "\"#cccc00\" if row[\"Status\"] == \"Pending\" else \\\n",
    "\"#cc0000\" if row[\"Status\"] == \"Open\" else \\\n",
    "\"#0000cc\" if row[\"Status\"] == \"Assigned\" else \"#444444\"\n",
    "popup = lambda row: str(row)\n",
    "plotpts(df2k, color=cfun, popup=popup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lead: analyse precinct efficiency, \"time to solve\", \"solving rate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Vehicle Type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Descriptor\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Descriptor\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neighborhood rating system, show subscores for:\n",
    "\n",
    "- noise disturbance (banging/pounding, loud party etc.)\n",
    "- infrastructure condition (pothole descriptor, road condition/street light condition etc.)\n",
    "- private residence condition (rat sighting, vermin, mold, etc.)\n",
    "- safety (mix with 911?)\n",
    "\n",
    "and create a general index too. mix with property prices to find overvalued/undervalued properties with predictive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Complaint type classification\n",
    "\n",
    "We manually classified complaint types into the following categories to use at a later point:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"complaint_type_cls_legend.json\") as fp_legend:\n",
    "    ct_legend = json.load(fp_legend)\n",
    "ct_legend   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following classification was performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"complaint_type_cls.json\") as fp_data:\n",
    "    ct_data = json.load(fp_data)\n",
    "ct_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Complaint Type Class Index\"] = df[\"Complaint Type\"].apply(lambda k: ct_data[k])\n",
    "ct_legend_inv = {v:k for k, v in ct_legend.items()}\n",
    "df[\"Complaint Type Class\"] = df[\"Complaint Type Class Index\"].apply(lambda k: ct_legend_inv[k])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Complaint Type Class\"].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Plotting GeoJSON NTAs with some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import json\n",
    "\n",
    "m = folium.Map(location=(40.730610, -73.935242), zoom_start=11)\n",
    "\n",
    "with open(\"data/Neighborhood Tabulation Areas.geojson\") as fp:\n",
    "    geojson = json.load(fp)\n",
    "    \n",
    "for feature in geojson[\"features\"]:\n",
    "    #print(feature[\"properties\"][\"ntacode\"])\n",
    "    gj = folium.GeoJson(feature, name=feature[\"properties\"][\"ntaname\"],\n",
    "                       style_function=lambda feature: {\n",
    "        'fillColor': \"#00ff00\",\n",
    "        'color' : \"#ff0000\",\n",
    "        'weight' : 1,\n",
    "        'fillOpacity' : 0.5,\n",
    "        })\n",
    "    folium.Popup(feature[\"properties\"][\"ntaname\"] + '\\n' + feature[\"properties\"][\"ntacode\"]).add_to(gj)\n",
    "    gj.add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PT_1 = (40.627, -73.966)\n",
    "PT_2 = (40.627, -73.986)\n",
    "\n",
    "m = folium.Map(location=(40.730610, -73.935242), zoom_start=11)\n",
    "\n",
    "with open(\"data/Neighborhood Tabulation Areas.geojson\") as fp:\n",
    "    geojson = json.load(fp)\n",
    "    \n",
    "for feature in geojson[\"features\"]:\n",
    "    #print(feature[\"properties\"][\"ntacode\"])\n",
    "    gj = folium.GeoJson(feature, name=feature[\"properties\"][\"ntaname\"],\n",
    "                       style_function=lambda feature: {\n",
    "        'fillColor': \"#00ff00\",\n",
    "        'color' : \"#ff0000\",\n",
    "        'weight' : 1,\n",
    "        'fillOpacity' : 0.5,\n",
    "        })\n",
    "    folium.Popup(feature[\"properties\"][\"ntaname\"] + '\\n' + feature[\"properties\"][\"ntacode\"]).add_to(gj)\n",
    "    gj.add_to(m)\n",
    "    break\n",
    "\n",
    "folium.Marker(PT_1).add_to(m)\n",
    "folium.Marker(PT_2).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapely.geometry as sg\n",
    "\n",
    "geometry = geojson[\"features\"][0][\"geometry\"]\n",
    "pt_1 = sg.Point(PT_1)\n",
    "pt_2 = sg.Point(PT_2)\n",
    "polygon = sg.shape(geometry)\n",
    "print(polygon.contains(pt_1))\n",
    "print(polygon.contains(pt_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Olivier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "import pickle as pk\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/311_Service_Requests_from_2010_to_Present_small.pkl\", 'rb') as f:\n",
    "    df = pk.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Complaint Type'].cat.remove_unused_categories(inplace=True)\n",
    "df['Created Date'] = pd.to_datetime(df['Created Date'])\n",
    "df['Closed Date'] = pd.to_datetime(df['Closed Date'])\n",
    "df = df.assign(Quantity=pd.Series(np.ones(len(df.index))).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_season(col=\"Complaint Type\", num_labels=30, plotting=\"plt\", frequency=\"M\", date_grouper=\"Created Date\"):\n",
    "    if plotting not in [\"plt\", \"plotly\"]:\n",
    "        raise NotImplementedError(\"Invalid plotting method\")\n",
    "    ### compute complaint types with most occurences\n",
    "    most_freq_labels = [x for x in df[col].value_counts().keys().values[:num_labels]]\n",
    "\n",
    "    ### group requests by month\n",
    "    season = df.groupby([col, pd.Grouper(key=date_grouper, freq=frequency)])['Quantity']\\\n",
    "        .sum().reset_index().sort_values(date_grouper)\n",
    "    season.fillna({'Quantity': 0}, inplace=True)\n",
    "\n",
    "\n",
    "    ### get corresponding times and quantities \n",
    "    times = [season[season[col] == c][date_grouper] for c in most_freq_labels]\n",
    "    qts = [season[season[col] == c]['Quantity'] for c in most_freq_labels]\n",
    "\n",
    "    ### plot\n",
    "    if plotting.lower() == \"plt\":\n",
    "        plt.figure(figsize=(16,8))\n",
    "        pal = sns.color_palette(\"Set1\")\n",
    "        plt.stackplot(times[0], qts, labels=most_freq_labels, colors=pal)\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        plt.tight_layout()\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Count of requests\")\n",
    "        plt.title(col)\n",
    "        plt.show();\n",
    "    elif plotting.lower() == \"plotly\":\n",
    "        fig = go.Figure({\"layout\": {\"title\": {\"text\": col}}})\n",
    "        for lab, q, t in zip(most_freq_labels, qts, times):\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=t, y=q,\n",
    "                hoverinfo='x+y',\n",
    "                mode='lines',\n",
    "                name=lab,\n",
    "                stackgroup='one' # define stack group\n",
    "            ))\n",
    "        fig.show()\n",
    "    else:\n",
    "        raise NotImplementedError(\"Woups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_season('Complaint Type', 100, plotting=\"plotly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_season(\"Agency\", 20, plotting=\"plotly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_season(\"Descriptor\", 30, plotting=\"plotly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "Around March-April 2014, no more _Heating_ entries are registered, but _HEAT/HOT WATER_ start to appear. It's probable it was replaced.\n",
    "\n",
    "Heating problems are obviously more reported during winter, peaking between December and January of each year.\n",
    "\n",
    "Similarly, _Street Conditions_ are more reported in spring, around March. \n",
    "\n",
    "Noise knows 2 peaks and 2 pits: the heart of summer and winter seem calmer. Probably because less people are in the street, being mostly on holiday (or inside, during winter). Meanwhile spring and autumn have more people around, creating more nuisance.\n",
    "\n",
    "Interestingly, while most of requests are gradually growing (due to the service being more and more popular), some manage to keep a steady level, notably Sewers.\n",
    "\n",
    "Sewers have another funny feature: August 2011 raises from the usual 2-3k monthly requests to 8k requests. It is possible that the reason for that is a light earthquake, that happend August 22nd, possibly disrupting the sewers system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Julien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path_311 = \"data/extract.csv\"\n",
    "path_nta = \"data/Neighborhood Tabulation Areas.geojson\"\n",
    "path_nta_population = \"data/New_York_City_Population_By_Neighborhood_Tabulation_Areas.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311 = pd.read_csv(path_311)\n",
    "df_nta_population = pd.read_csv(path_nta_population)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the unique columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some columns are specific to certain complaint types: **Landmark**, **Park Facility Name**, **Vehicle Type**, **Taxi Company Borough**, **Taxi Pick Up Location**, **Bridge Highway Name**, **Bridge Highway Direction**, **Road Ramp**, **Bridge Highway Segment**. If those fields are empty for most complaints, we might as well drop the corresponding columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complaints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"Complaint Type\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints = df_311[\"Complaint Type\"].unique().tolist()\n",
    "complaints.sort()\n",
    "print(complaints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One entry has to be removed: './validate_form.php'. Should check for other outliers in the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaint_count = df_311.groupby(\"Complaint Type\") \\\n",
    "                        .count()[\"Unique Key\"] \\\n",
    "                        .sort_values(ascending=False) \n",
    "      \n",
    "complaint_count.plot.barh(x=\"Complaint Type\", y=\"Unique Key\", logx=True, figsize=(15, 40));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the top 30 complaints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaint_count[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, some complaints are redundant and should be grouped. (HEATING, PAINT/PLASTER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"Created Date\"] = pd.to_datetime(df_311[\"Created Date\"])\n",
    "df_311[\"Closed Date\"] = pd.to_datetime(df_311[\"Closed Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = df_311[\"Created Date\"].apply(lambda d: d.year).rename(\"cyear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years.groupby(years) \\\n",
    "     .count() \\\n",
    "     .sort_values(ascending=False) \\\n",
    "     .plot.barh(x=\"cyear\", y=\"Count\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values were randomly sampled so there is a margin of error, and 2019 is not over, but it seems like there is a linear increase in the number of 311 requests - the service gets more popular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[df_311[\"Location\"].isna()].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"Incident Zip\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[df_311[\"Incident Zip\"].isna()].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the type of complaint, location might be undefined (traffic light at intersection, literature request)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[~(df_311[\"Incident Zip\"].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[~(df_311[\"Longitude\"].isna())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do have the exact location for most of the requests (>80%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that **X Coordinate (State Plane)** / **Y Coordinate (State Plane)** might be more precise than **Longitude** / **Latitude**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_count = df_311.groupby(\"Open Data Channel Type\") \\\n",
    "                        .count()[\"Unique Key\"] \\\n",
    "                        .sort_values(ascending=False) \n",
    "      \n",
    "channel_count.plot.barh(x=\"Channel\", y=\"Count\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most requests are still submitted through a phone call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focus on certain complaints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look again at the top 30 complaints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaint_count[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things that stand out: \n",
    "- Noise most recurrent complaint, different types of noise\n",
    "- Followed by Water / Heating systems / Plumbing / Sewer\n",
    "- Things that refer to the bad condition of something: Street Condition, Street Light Condition, Traffic Signal Condition, Unsanitary conditions, Dirty Conditions, Sanitation Condition, Sidewalk Condition and many more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of things that could be done:\n",
    "- Per Neighborhood Tabulation Area (NTA): Top 30 Problems, frequency w.r.t the size of the NTA, look into trends\n",
    "- Average time to resolution over the years (compare NTAs, agencies)\n",
    "- Word cloud per NTA\n",
    "- How well prepared for / resistent to external events, such as blizzards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: missing something interesting, that goes beyond the simple analysis. Make use of another dataset? Do something for social good: either help agencies fix their problems more efficiently (redraw borders of NTAs, encourage communication between departments), or give people some advice (Fix your heating system before the winter, because there is a high probability of failure - Do you really want to move to that neighborhood? It has a couple of problems, here is a better one, which is much more silent). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add NTA information to complaints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from shapely.geometry import shape, Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load GeoJSON file containing NTAs\n",
    "with open(path_nta) as f:\n",
    "    js = json.load(f)\n",
    "\n",
    "def get_nta(location_str):\n",
    "    \"\"\"Retrieves the NTA of a given location in NYC.\n",
    "    Args:\n",
    "        location_str (String): location associated to the complaint, that is '(latitude, longitude)'\n",
    "    Returns:\n",
    "        String: code of the NTA the location is part of, or NaN if the location is not within an NTA\n",
    "    \"\"\"\n",
    "    location = eval(location_str)\n",
    "    latitude = location[0]\n",
    "    longitude = location[1]\n",
    "    point = Point(longitude, latitude)\n",
    "    for feature in js[\"features\"]:\n",
    "        polygon = shape(feature[\"geometry\"])\n",
    "        if polygon.contains(point):\n",
    "            nta_code = feature[\"properties\"][\"ntacode\"]\n",
    "            return nta_code\n",
    "    return np.nan\n",
    "    \n",
    "get_nta('(40.77382381576739,-73.95411117806607)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"NTA\"] = np.nan\n",
    "df_311.loc[df_311.Location.notnull(), \"NTA\"] = df_311.Location[df_311.Location.notnull()].apply(get_nta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Added the NTA to {df_311.NTA.notnull().sum()} complaints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_loc_complaints = df_311[df_311.NTA.isnull()]\n",
    "complaint_count = no_loc_complaints.groupby(\"Complaint Type\") \\\n",
    "                                   .count()[\"Unique Key\"] \\\n",
    "                                   .sort_values(ascending=False) \n",
    "      \n",
    "complaint_count.head(10).plot.barh(x=\"Complaint Type\", y=\"Count\", figsize=(15, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Street Lights** / **Traffic Signal** are often at intersections, which don't have an exact address. We also have complaints which don't refer to a specific location such as **Benefit Card Replacement**. We might have to watch out for **Street Light Condition** complaints, because they also are among the most frequent complaints in the raw dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse noise based on time of the day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter noise complaints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints_lowercase = list(map(lambda x: (x, x.lower()), complaints))\n",
    "noise_complaint_types = [complaint[0] for complaint in complaints_lowercase if \"noise\" in complaint[1]]\n",
    "print(noise_complaint_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_complaints = df_311[df_311[\"Complaint Type\"].isin(noise_complaint_types)]\n",
    "complaint_count = noise_complaints.groupby(\"Complaint Type\") \\\n",
    "                                  .count()[\"Unique Key\"] \\\n",
    "                                  .sort_values(ascending=False) \n",
    "      \n",
    "complaint_count.plot.barh(x=\"Complaint Type\", y=\"Unique Key\", logx=True, figsize=(15, 10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_requests = \"data/311_Service_Requests_from_2010_to_Present_sorted.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start = datetime(2019, 8, 5)\n",
    "end = datetime(2019, 10, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = df_311.columns.tolist()[:-1]\n",
    "sorted_df = pd.read_csv(sorted_requests, header=None, names=names, nrows=500000)\n",
    "sorted_df = sorted_df[sorted_df[\"Complaint Type\"].isin(noise_complaint_types)]\n",
    "sorted_df[\"Created Date\"] = pd.to_datetime(sorted_df[\"Created Date\"])\n",
    "sorted_df = sorted_df[(start <= sorted_df[\"Created Date\"]) & (sorted_df[\"Created Date\"] < end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df[\"Created Day\"] = sorted_df[\"Created Date\"].dt.date\n",
    "sorted_df[\"Created Hour\"] = sorted_df[\"Created Date\"].dt.hour\n",
    "sorted_df[\"Created Weekday\"] = sorted_df[\"Created Date\"].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_complaints = sorted_df.groupby(\"Created Day\")[\"Created Day\"] \\\n",
    "                            .count()\n",
    "    \n",
    "daily_complaints.plot.barh(figsize=(15,30));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_start = datetime(2019, 9, 2)\n",
    "month_end = datetime(2019, 9, 30)\n",
    "\n",
    "daily_complaints = sorted_df[(month_start <= sorted_df[\"Created Date\"]) & (sorted_df[\"Created Date\"] < month_end)] \\\n",
    "                            .groupby(['Created Day', 'Complaint Type'])['Created Day'] \\\n",
    "                            .count() \\\n",
    "                            .unstack('Complaint Type') \\\n",
    "                            .fillna(0)\n",
    "\n",
    "ax = daily_complaints.plot(kind='bar', stacked=True, figsize=(30,10), rot=45)\n",
    "ax.set_title(\"Noise complaints over the covered period\")\n",
    "ax.set_xlabel(\"Hour\")\n",
    "ax.set_ylabel(\"Count\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seasonality is obvious, there is a weekly pattern, with a peak in noise complaints over the weekend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_complaints = sorted_df.groupby(['Created Hour', 'Complaint Type'])['Created Hour'].count().unstack('Complaint Type').fillna(0)\n",
    "ax = hourly_complaints.plot(kind='bar', stacked=True, figsize=(20,10), rot=0)\n",
    "ax.set_title(\"Aggregated noise complaints over the hours of a day\")\n",
    "ax.set_xlabel(\"Hour\")\n",
    "ax.set_ylabel(\"Count\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a daily pattern, with lots of noise complaints during the night. We might want to look at the pattern we get when aggregating based on the hour and the weekday, to see if there are differences and validate our current approach (disregard weekday). We expect to have a similar pattern every day, but with different numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_complaints = sorted_df.groupby(['Created Weekday', 'Created Hour'])[['Unique Key']].count().reset_index()#.unstack('Complaint Type').fillna(0)  hourly_complaints\n",
    "#ax = hourly_complaints.plot(kind='bar', stacked=True, figsize=(20,10), rot=0)\n",
    "#ax.set_title(\"Aggregated noise complaints over the hours of a day\")\n",
    "#ax.set_xlabel(\"Hour\")\n",
    "#ax.set_ylabel(\"Count\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_complaints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "g = sns.catplot(x='Created Hour', y='Unique Key', col='Created Weekday', data=hourly_complaints, kind='bar', col_wrap=2, sharey=False, height=4, aspect=2)\n",
    "g = g.set_ylabels(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really different shapes, there are just peaks on Friday and Saturday night that must be taken into account and have an effect on the last 3 plots. Question arises: Is it just party-related / nightly complaints that are responsible for the weekend peaks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(x='Created Weekday', y='Unique Key', col='Created Hour', data=hourly_complaints, kind='bar', col_wrap=2, sharey=False, height=4, aspect=2)\n",
    "g = g.set_ylabels(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only are there more nighttime noise-complaints on the Weekend, there are also more daytime complaints, probably because people tend to be at home over the weekend, where they are more likely to be disturbed than at work. This is something that should also be taken into account when **reasoning about other types of complaints**, i. e. when interpretating a number of complaints we should always think about factors like the weekday, the hour, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment on the plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can be done with noise:\n",
    "- Provide general method to analyse seasonality, that can be used for any column\n",
    "- Compute daily mean, then spot outliers \n",
    "- Compute hourly mean (weekday), then spot outliers\n",
    "- Look at noise distribution across NTAs, normalise with NTA population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noise seems to have quite a high variance, CIs wouldn't give much insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse noise based on location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to identify noisy NTAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df[\"NTA\"] = np.nan\n",
    "sorted_df.loc[sorted_df.Location.notnull(), \"NTA\"] = sorted_df.Location[sorted_df.Location.notnull()].apply(get_nta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load NTA population sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nta_pop_df = pd.read_csv(path_nta_population)\n",
    "nta_pop_df = nta_pop_df[nta_pop_df.Year == 2010].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nta_complaints = sorted_df.groupby(\"NTA\")[[\"Unique Key\"]] \\\n",
    "                          .count() \\\n",
    "                          .rename(columns={\"Unique Key\": \"Count\"})\n",
    "\n",
    "nta_complaints_extended = nta_complaints.merge(nta_pop_df.set_index(\"NTA Code\"), left_index=True, right_index=True)\n",
    "nta_complaints_extended[\"Normalized Count\"] = nta_complaints_extended[\"Count\"] / nta_complaints_extended[\"Population\"]\n",
    "nta_complaints_extended.loc[nta_complaints_extended.Population == 0, \"Normalized Count\"] = np.nan\n",
    "nta_complaints_extended = nta_complaints_extended.reset_index().set_index(\"NTA Name\").drop(columns=[\"level_0\"])\n",
    "nta_complaints_extended.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nta_complaints_extended[\"NC Rank\"] = nta_complaints_extended[\"Normalized Count\"].rank(method=\"min\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nta_complaints_extended[[\"Count\", \"Population\", \"NC Rank\"]].sort_values(by=\"Count\", ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nta_complaints_extended[[\"Normalized Count\", \"Population\"]].sort_values(by=\"Normalized Count\", ascending=False)[5:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nta_complaints_extended[nta_complaints_extended.Population < 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nta_complaints_extended[[\"Count\"]].sum().tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at type of complaints in top 20 NTAs (based on normalized number of complaints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada] *",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
