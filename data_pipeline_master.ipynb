{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "path_311 = \"data/311_Service_Requests_from_2010_to_Present.csv\"\n",
    "path_nta = \"data/Neighborhood Tabulation Areas.geojson\"\n",
    "path_nta_population = \"data/New_York_City_Population_By_Neighborhood_Tabulation_Areas.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311 = pd.read_csv(path_311)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that \"Unique Key\" column can be an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"Unique Key\"].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311.set_index(\"Unique Key\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check what complaint types do not have \"Location\" set. If there would be few of those or the categories would be of no interest to us, we would have dropped them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[df_311[\"Location\"].isna()].groupby(\"Complaint Type\").count()[\"Unique Key\"].sort_values()[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the State Plane coordinates as they are just a different format of the coordinates present in Latitude and Longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311.drop(columns=[\"X Coordinate (State Plane)\", \"Y Coordinate (State Plane)\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the \"Location\" column as it is just the Latitude and Longitude columns combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311.drop(columns=\"Location\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After dropping Location and X/Y Coordinates in State Plane "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We realize that some of the complaint types are incorrct. Many of such entries categories appear only once in the dataset. We drop those entries as we cannot interpret such complaints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_complaints = list(df_311[\"Complaint Type\"].value_counts(ascending=True)[\n",
    "    df_311[\"Complaint Type\"].value_counts(ascending=True) == 1].index)\n",
    "df_311 = df_311[~df_311[\"Complaint Type\"].isin(invalid_complaints)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casting fields to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_created_dates = df_311[\"Created Date\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_closed_dates = df_311[\"Closed Date\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_closed_dates = list(pd.DataFrame(unique_closed_dates)[0].dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will verify if the fields with dates have proper formatting. This still does not guarantee that they are logically correct, that is in some expected range, but tells us whether or not we would be able to parse them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt='01/17/2026 10:12:35 PM'\n",
    "\n",
    "# MMDDYYYY 1\n",
    "re1='((?:[0]?[1-9]|[1][012])[-:\\\\/.](?:(?:[0-2]?\\\\d{1})|(?:[3][01]{1}))[-:\\\\/.](?:(?:[1]{1}\\\\d{1}\\\\d{1}\\\\d{1})|(?:[2]{1}\\\\d{3})))(?![\\\\d])'\n",
    "# White Space 1\n",
    "re2='( )'\t\n",
    "# HourMinuteSec\n",
    "re3='((?:(?:[0-1][0-9])|(?:[2][0-3])|(?:[0-9])):(?:[0-5][0-9])(?::[0-5][0-9])?(?:\\\\s?(?:am|AM|pm|PM))?)'\n",
    "\n",
    "rg = re.compile(re1+re2+re3,re.IGNORECASE|re.DOTALL)\n",
    "\n",
    "def try_matching(txt):\n",
    "    m = rg.search(str(txt))\n",
    "    if not m:\n",
    "        print(\"NOT A MATCH!\")\n",
    "        print(txt)\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_created = list(map(lambda x: try_matching(x), unique_created_dates))\n",
    "sum(mapped_created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_closed = list(map(lambda x: try_matching(x), unique_closed_dates))\n",
    "sum(mapped_closed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one entry among closed dates that is in the third millenium. We assume time travel is impossible, so the complaint couldn't have been closed in the future. Dropping the entry with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311 = df_311[df_311[\"Closed Date\"] != \"03/30/3027 12:00:00 AM\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing the dates and casting them to pandas datetime type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"Created Date\"] = pd.to_datetime(df_311[\"Created Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"Closed Date\"] = pd.to_datetime(df_311[\"Closed Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"Created Date\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the \"Created Date\" column appears to contain only dates that are possible (within reasonable range. Moreover, we clearly see an increase in the number of complaints overall since the year 2012. This might be either due to growing popularity of 311, increase in population or worsening conditions in the city. It might be a combination of those factors, so we do not draw final conclusions yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"Closed Date\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly there is something wrong with the values of ticket closed date. There are entries that have the dates set in 1900s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[df_311[\"Closed Date\"] < datetime.datetime(2009, 1, 1)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that those which do not have \"Status\" set to closed should not have the \"Closed Date\" set in the first place as the only other status present among them is \"Pending\". Thus we set their \"Closed Date\" to NaN. We remove the entries which had the status closed and \"Closed date\" set before 2010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[(df_311[\"Closed Date\"] < datetime.datetime(2010, 1, 1)) & (df_311[\"Status\"] == \"Closed\")][\"Unique Key\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311.drop(df_311[(df_311[\"Closed Date\"] < datetime.datetime(2010, 1, 1)) & (df_311[\"Status\"] == \"Closed\")].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the issues we have spotted is the fact that there are entries with \"Closed Date\" before \"Created Date\". We assume this might be a way of dealing with complaints submitted for the problems that were already resolved. Those may be also plain mistakes. We decide to drop all such rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311.drop(df_311[df_311[\"Closed Date\"] < df_311[\"Created Date\"]].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"Closed Date\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also decide to remove the rows that have \"Closed Date\" after today. This is because one expects the tickets with \"Closed Date\" present to be actually closed already. There is few such cases, so it should not pose a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[df_311[\"Closed Date\"] > datetime.datetime.today()][\"Unique Key\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311.drop(df_311[df_311[\"Closed Date\"] > datetime.datetime.today()].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two more columns with dates in them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"Due Date\"] = pd.to_datetime(df_311[\"Due Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"Due Date\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that the Due Dates set in 1900s are incorrect and replace them by NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[df_311[\"Due Date\"] < datetime.datetime(2010, 1, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"Due Date\"] = df_311[\"Due Date\"].replace(datetime.datetime(1900, 1, 2), np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"Due Date\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_resolution_dates = df_311[\"Resolution Action Updated Date\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(map(lambda x: try_matching(x), unique_resolution_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_without_incorrect = df_311[~df_311[\"Resolution Action Updated Date\"].isin(incorrect_dates)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_without_incorrect[\"Resolution Action Updated Date\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_dates = [\"10/30/2926 11:51:00 AM\", \"05/25/2510 10:15:00 AM\", \"03/06/2927 12:30:00 PM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"Resolution Action Updated Date\"] = df_311[~df_311[\"Resolution Action Updated Date\"].isin(incorrect_dates)][\"Resolution Action Updated Date\"]\n",
    "df_311[\"Resolution Action Updated Date\"] = pd.to_datetime(df_311[\"Resolution Action Updated Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"Resolution Action Updated Date\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the rows where the Resolution Action Updated Date is in the future. By definition it should be the date when agency has last updated the entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311.drop(df_311[df_311[\"Resolution Action Updated Date\"] > datetime.datetime.today()].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"Resolution Action Updated Date\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to drop the entries with \"Resolution Action Updated Date\" before 2010 as that is the date where the data has started being collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311.drop(df_311[df_311[\"Resolution Action Updated Date\"] < datetime.datetime(2010, 1, 1)].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[\"Resolution Action Updated Date\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casting to \"category\" type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that many of the columns can take only one of a small set of possible values. We cast them to \"category\" type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_columns = [\"Agency\", \"Agency Name\", \"Complaint Type\", \"Descriptor\", \"Location Type\",\n",
    "                    \"Community Board\", \"Address Type\", \"City\", \"Landmark\", \"Facility Type\", \"Status\",\n",
    "                    \"Resolution Description\", \"Borough\", \"Open Data Channel Type\", \"Park Facility Name\",\n",
    "                    \"Park Borough\", \"Vehicle Type\", \"Taxi Company Borough\", \"Bridge Highway Direction\", \"Road Ramp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[category_columns] = df_311[category_columns].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311.to_pickle(\"data/311_Service_Requests_from_2010_to_Present_small.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df_311 = pd.read_pickle(\"data/311_Service_Requests_from_2010_to_Present_small.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate wordcloud from Complaint Type, Descriptor, and Resolution Descriptor. Those can be narrowed down later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def create_wordcloud(pandas_series, max_words=2000, max_font_size=None, mask=None, word_level=False, filepath=None):\n",
    "    wordcloud = WordCloud(\n",
    "        width=3000,\n",
    "        height=2000,\n",
    "        max_words=max_words,\n",
    "        max_font_size=max_font_size,\n",
    "        background_color='white',\n",
    "        stopwords=STOPWORDS,\n",
    "        random_state=1,\n",
    "        mask=mask,\n",
    "        contour_width=3,\n",
    "        contour_color='black'\n",
    "    )\n",
    "    \n",
    "    text = pandas_series.astype(str).values\n",
    "    processed_text = wordcloud.process_text(\" \".join(text))\n",
    "    wordcloud.generate_from_frequencies(processed_text)\n",
    "    fig = plt.figure(\n",
    "        figsize = (40, 30),\n",
    "        facecolor = 'k',\n",
    "        edgecolor = 'k')\n",
    "    plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout(pad=0)\n",
    "    if filepath:\n",
    "        wordcloud.to_file(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_mask = np.array(Image.open(\"data/NYC_silhouette.png\"))\n",
    "nyc_mask[nyc_mask > 0] = 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_wordcloud(df_311[\"Complaint Type\"], mask=nyc_mask, filepath=\"data/complaint_type_wc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_wordcloud(df_311[\"Resolution Description\"], filepath=\"data/resolution_description_wc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_wordcloud(df_311[\"Descriptor\"], filepath=\"data/resolution_descriptor_wc.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating how agencies are split to take care of smaller regions in NYC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df_311 = pd.read_pickle(\"data/311_Service_Requests_from_2010_to_Present_small.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school_agencies = [agency for agency in list(df_311[\"Agency Name\"].unique()) if \"School - \" in agency]\n",
    "len(school_agencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[df_311[\"Agency Name\"].isin(school_agencies)][\"Agency\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the agencies are schools. All of them are mapped to agency \"DOE\". Schools are divided into NYC school districts. The map of such districts is available https://data.cityofnewyork.us/Education/School-Districts/r8nu-ymqj , but we don't see a dataset that could map 3-1-1 Agency Names to school districts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_school_agencies = [agency for agency in list(df_311[\"Agency Name\"].unique()) if not \"School - \" in agency]\n",
    "len(not_school_agencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311[df_311[\"Agency\"] == \"NYPD\"][\"Agency Name\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that a single \"Agency\" entry maps to multiple \"Agency Name\" entries that in some cases are equivallent, such as \"NYPD\" and \"New York City Police Department\". This indicates that we should first look at the \"Agencies\" themselves as \"Agency Names\" may not be meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_311[\"Agency\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NYPD is split into Police Precints: https://data.cityofnewyork.us/Public-Safety/Police-Precincts/78dh-3ptz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_311[df_311[\"Agency\"] == \"OMB\"][\"Agency Name\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DSNY (Department of Sanitation) districts: https://data.cityofnewyork.us/City-Government/DSNY-Districts/6j86-5s7z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FDNY (Fire Division). Battallions: https://data.cityofnewyork.us/Public-Safety/Fire-Battalions/uh7r-6nya , divisions (coarse grained): https://data.cityofnewyork.us/Public-Safety/Fire-Divisions/hkpx-aaxc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, we have found geographical subdivisions for DOE, NYPD, DSNY and FDNY."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Louis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_PATH = \"data/extract.csv\"\n",
    "path_nta = \"data/Neighborhood Tabulation Areas.geojson\"\n",
    "path_nta_population = \"data/New_York_City_Population_By_Neighborhood_Tabulation_Areas.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(SAMPLE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check that the Unique Key truly is unique, and that no NaN's appear in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df[\"Unique Key\"].is_unique\n",
    "assert not df[\"Unique Key\"].isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df.columns:\n",
    "    if \"Date\" in c:\n",
    "        print(\"Converting\", c)\n",
    "        df[c] = pd.to_datetime(df[\"Created Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Created Day\"] = df[\"Created Date\"].apply(lambda d: d.day)\n",
    "df[\"Created Month\"] = df[\"Created Date\"].apply(lambda d: d.month)\n",
    "df[\"Created Year\"] = df[\"Created Date\"].apply(lambda d: d.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatbarplot(value_counts):\n",
    "    max_v = value_counts.max()\n",
    "    min_v = value_counts.min()\n",
    "    clrs = {k:np.array([(v-min_v)/(max_v-min_v), 0., 0.]) for k, v in zip(value_counts.keys(), value_counts.values)}\n",
    "    sns.barplot(x=value_counts.keys(), y=value_counts.values, palette=clrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatbarplot(df[\"Created Month\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatbarplot(df[\"Created Year\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatbarplot(df[\"Created Day\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Agency\"].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ct_vcs = df[\"Complaint Type\"].value_counts()\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "ct_vcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ct_vcs.head().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotpts(geodf, color=\"#ff0000\", popup=\"None\"):\n",
    "    clean = geodf[geodf[\"Latitude\"].notnull()]\n",
    "    clean = clean[clean[\"Longitude\"].notnull()]\n",
    "    m = folium.Map(location=[clean[\"Latitude\"].values[0], clean[\"Longitude\"].values[0]], zoom_start=10)\n",
    "    #for c in zip(clean[\"Latitude\"].values, clean[\"Longitude\"].values):\n",
    "    for i, row in clean.iterrows():\n",
    "        folium.CircleMarker(location=(row[\"Latitude\"], row[\"Longitude\"]),\n",
    "                            radius= 1,\n",
    "                            color=color(row) if callable(color) else color,\n",
    "                            popup=popup(row) if callable(popup) else popup,\n",
    "                            fill=True).add_to(m)\n",
    "    return m\n",
    "plotpts(df[df[\"Complaint Type\"] == \"Noise - Residential\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfun = lambda row: \"#aa3300\" if row[\"Complaint Type\"] == \"Noise - Residential\" else \\\n",
    "\"#00aa33\" if row[\"Complaint Type\"] == \"Street Condition\" else \\\n",
    "\"#aa0066\" if row[\"Complaint Type\"] == \"Street Light Condition\" else \\\n",
    "\"#cccc00\" if row[\"Complaint Type\"] == \"HEAT/HOT WATER\" else \\\n",
    "\"#3300aa\" if row[\"Complaint Type\"] == \"Illegal Parking\" else \"#444444\"\n",
    "popup = lambda row: str(popup)\n",
    "df2k = df.sample(n=2000, random_state=1)\n",
    "plotpts(df2k, color=cfun, popup=popup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Out of\", df.shape[0], \"entries:\")\n",
    "facility_df = df[df[\"Facility Type\"].notnull()]\n",
    "print(facility_df.shape[0], \"have associated facility type.\")\n",
    "facility_df[\"Facility Type\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3 disctinct associated facility type. Mostly Precinct, then DSNY Garage then a few School."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Open Data Channel Type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Status\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Borough\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2k = df.sample(n=2000, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfun = lambda row: \"#00cc00\" if row[\"Status\"] == \"Closed\" else \\\n",
    "\"#cccc00\" if row[\"Status\"] == \"Pending\" else \\\n",
    "\"#cc0000\" if row[\"Status\"] == \"Open\" else \\\n",
    "\"#0000cc\" if row[\"Status\"] == \"Assigned\" else \"#444444\"\n",
    "popup = lambda row: str(row)\n",
    "plotpts(df2k, color=cfun, popup=popup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lead: analyse precinct efficiency, \"time to solve\", \"solving rate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Vehicle Type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Descriptor\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Descriptor\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neighborhood rating system, show subscores for:\n",
    "\n",
    "- noise disturbance (banging/pounding, loud party etc.)\n",
    "- infrastructure condition (pothole descriptor, road condition/street light condition etc.)\n",
    "- private residence condition (rat sighting, vermin, mold, etc.)\n",
    "- safety (mix with 911?)\n",
    "\n",
    "and create a general index too. mix with property prices to find overvalued/undervalued properties with predictive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Complaint type classification\n",
    "\n",
    "We manually classified complaint types into the following categories to use at a later point:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"complaint_type_cls_legend.json\") as fp_legend:\n",
    "    ct_legend = json.load(fp_legend)\n",
    "ct_legend   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following classification was performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"complaint_type_cls.json\") as fp_data:\n",
    "    ct_data = json.load(fp_data)\n",
    "ct_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Complaint Type Class Index\"] = df[\"Complaint Type\"].apply(lambda k: ct_data[k])\n",
    "ct_legend_inv = {v:k for k, v in ct_legend.items()}\n",
    "df[\"Complaint Type Class\"] = df[\"Complaint Type Class Index\"].apply(lambda k: ct_legend_inv[k])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Complaint Type Class\"].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Plotting GeoJSON NTAs with some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import json\n",
    "\n",
    "m = folium.Map(location=(40.730610, -73.935242), zoom_start=11)\n",
    "\n",
    "with open(\"data/Neighborhood Tabulation Areas.geojson\") as fp:\n",
    "    geojson = json.load(fp)\n",
    "    \n",
    "for feature in geojson[\"features\"]:\n",
    "    #print(feature[\"properties\"][\"ntacode\"])\n",
    "    gj = folium.GeoJson(feature, name=feature[\"properties\"][\"ntaname\"],\n",
    "                       style_function=lambda feature: {\n",
    "        'fillColor': \"#00ff00\",\n",
    "        'color' : \"#ff0000\",\n",
    "        'weight' : 1,\n",
    "        'fillOpacity' : 0.5,\n",
    "        })\n",
    "    folium.Popup(feature[\"properties\"][\"ntaname\"] + '\\n' + feature[\"properties\"][\"ntacode\"]).add_to(gj)\n",
    "    gj.add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PT_1 = (40.627, -73.966)\n",
    "PT_2 = (40.627, -73.986)\n",
    "\n",
    "m = folium.Map(location=(40.730610, -73.935242), zoom_start=11)\n",
    "\n",
    "with open(\"data/Neighborhood Tabulation Areas.geojson\") as fp:\n",
    "    geojson = json.load(fp)\n",
    "    \n",
    "for feature in geojson[\"features\"]:\n",
    "    #print(feature[\"properties\"][\"ntacode\"])\n",
    "    gj = folium.GeoJson(feature, name=feature[\"properties\"][\"ntaname\"],\n",
    "                       style_function=lambda feature: {\n",
    "        'fillColor': \"#00ff00\",\n",
    "        'color' : \"#ff0000\",\n",
    "        'weight' : 1,\n",
    "        'fillOpacity' : 0.5,\n",
    "        })\n",
    "    folium.Popup(feature[\"properties\"][\"ntaname\"] + '\\n' + feature[\"properties\"][\"ntacode\"]).add_to(gj)\n",
    "    gj.add_to(m)\n",
    "    break\n",
    "\n",
    "folium.Marker(PT_1).add_to(m)\n",
    "folium.Marker(PT_2).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapely.geometry as sg\n",
    "\n",
    "geometry = geojson[\"features\"][0][\"geometry\"]\n",
    "pt_1 = sg.Point(PT_1)\n",
    "pt_2 = sg.Point(PT_2)\n",
    "polygon = sg.shape(geometry)\n",
    "print(polygon.contains(pt_1))\n",
    "print(polygon.contains(pt_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "import pickle as pk\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/311_Service_Requests_from_2010_to_Present_small.pkl\", 'rb') as f:\n",
    "    df_311 = pk.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['Complaint Type'].cat.remove_unused_categories(inplace=True)\n",
    "df_311['Created Date'] = pd.to_datetime(df_311['Created Date'])\n",
    "df_311['Closed Date'] = pd.to_datetime(df_311['Closed Date'])\n",
    "df_311 = df.assign(Quantity=pd.Series(np.ones(len(df_311.index))).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_season(col=\"Complaint Type\", num_labels=30, plotting=\"plt\", frequency=\"M\", date_grouper=\"Created Date\"):\n",
    "    if plotting not in [\"plt\", \"plotly\"]:\n",
    "        raise NotImplementedError(\"Invalid plotting method\")\n",
    "    ### compute complaint types with most occurences\n",
    "    most_freq_labels = [x for x in df_311[col].value_counts().keys().values[:num_labels]]\n",
    "\n",
    "    ### group requests by month\n",
    "    season = df_311.groupby([col, pd.Grouper(key=date_grouper, freq=frequency)])['Quantity']\\\n",
    "        .sum().reset_index().sort_values(date_grouper)\n",
    "    season.fillna({'Quantity': 0}, inplace=True)\n",
    "\n",
    "\n",
    "    ### get corresponding times and quantities \n",
    "    times = [season[season[col] == c][date_grouper] for c in most_freq_labels]\n",
    "    qts = [season[season[col] == c]['Quantity'] for c in most_freq_labels]\n",
    "\n",
    "    ### plot\n",
    "    if plotting.lower() == \"plt\":\n",
    "        plt.figure(figsize=(16,8))\n",
    "        pal = sns.color_palette(\"Set1\")\n",
    "        plt.stackplot(times[0], qts, labels=most_freq_labels, colors=pal)\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        plt.tight_layout()\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Count of requests\")\n",
    "        plt.title(col)\n",
    "        plt.show();\n",
    "    elif plotting.lower() == \"plotly\":\n",
    "        fig = go.Figure({\"layout\": {\"title\": {\"text\": col}}})\n",
    "        for lab, q, t in zip(most_freq_labels, qts, times):\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=t, y=q,\n",
    "                hoverinfo='x+y',\n",
    "                mode='lines',\n",
    "                name=lab,\n",
    "                stackgroup='one' # define stack group\n",
    "            ))\n",
    "        fig.show()\n",
    "    else:\n",
    "        raise NotImplementedError(\"Woups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot per complaint type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_season('Complaint Type', 100, plotting=\"plotly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot per agency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_season(\"Agency\", 20, plotting=\"plotly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot per descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_season(\"Descriptor\", 30, plotting=\"plotly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "The above plots are extremely useful in notebook format. They allow to zoom, move, scroll, select one or multiple categories,take snapshots,... Sadly, the pictures taken by snapshot are not as flexible: we can't chose the size, have a better view of the axis, not chose the scroll level of the legend, on the right (it's always capped at the top). We tried to make clear snapshots and explain them, but nonetheless apologize for the inconvenience.\n",
    "## Observations\n",
    "\n",
    "Around March-April 2014, no more _Heating_ entries are registered, but _HEAT/HOT WATER_ start to appear. It's probable it was replaced.\n",
    "\n",
    "Heating problems are obviously more reported during winter, peaking between December and January of each year.\n",
    "\n",
    "![Heat/Hot water](images/heatwater.png)\n",
    "\n",
    "Tree also had a renaming: _Dead Trees_ to _Dead/Dying Trees_. Apparently, they are different!\n",
    "\n",
    "![Dead trees](images/deadtree.png)\n",
    "\n",
    "Similarly, _Street Conditions_ are more reported in spring, around March. \n",
    "\n",
    "![Street Conditions](images/streetconditions.png)\n",
    "\n",
    "\n",
    "Noise knows 2 peaks and 2 pits: the heart of summer and winter seem calmer. Probably because less people are in the street, being mostly on holiday (or inside, during winter). Meanwhile spring and autumn have more people around, creating more nuisance.\n",
    "\n",
    "![Noise](images/noise.png)\n",
    "\n",
    "\n",
    "Interestingly, while most of requests are gradually growing (due to the service being more and more popular), some manage to keep a steady level, notably Sewers.\n",
    "\n",
    "Sewers have another funny feature: August 2011 raises from the usual 2-3k monthly requests to 8k requests. It is possible that the reason for that is a light earthquake, that happend August 22nd, possibly disrupting the sewers system.\n",
    "\n",
    "![Sewers](images/sewer.png)\n",
    "\n",
    "Rodent appear mostly during summertime\n",
    "\n",
    "![Rodents](images/rodent.png)\n",
    "\n",
    "Illegal parking requests are strongly rising, more than the global rate. This would require additional analysis to draw conclusion, but we can suppose either more people park illegally, or people are specially interested in 3-1-1 as a way to denounce illegal parking.\n",
    "\n",
    "![Parking](images/illegalparking.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of noise complaints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Imports for this part\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from shapely.geometry import shape, Point\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Noise complaints** being among the most frequent complaints, we do a small analysis of that type of complaints, to get a better understanding of it and be able to re-use general analysis methods for other types of complaints. This analysis is not complete, but shows that we have an improved knowledge of our data, which will allow us to develop robust methods for the upcoming milestone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_311 = \"data/311_Service_Requests_from_2010_to_Present_small.pkl\"\n",
    "path_noise = \"data/311_Noise_Complaints.pkl\"\n",
    "path_nta = \"data/Neighborhood Tabulation Areas.geojson\"\n",
    "path_nta_population = \"data/New_York_City_Population_By_Neighborhood_Tabulation_Areas.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311 = pd.read_pickle(path_311)\n",
    "df_nta_population = pd.read_csv(path_nta_population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the noise complaints analysis we only need a few columns, the rest can be disregarded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_selection = ['Created Date', 'Complaint Type', 'Latitude', 'Longitude']\n",
    "noise_complaints = df_311[col_selection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints = df_311[\"Complaint Type\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints_lowercase = list(map(lambda x: (x, x.lower()), complaints))\n",
    "noise_complaint_types = [complaint[0] for complaint in complaints_lowercase if \"noise\" in complaint[1]]\n",
    "print(noise_complaint_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_complaint_types = noise_complaint_types[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_complaints = df_311[df_311[\"Complaint Type\"].isin(noise_complaint_types)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We focus on noise complaints over a period of 11 weeks, to get more precise information about short-term trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(2019, 8, 5)\n",
    "end = datetime(2019, 10, 21)\n",
    "noise_complaints = noise_complaints[(start <= noise_complaints[\"Created Date\"]) & (noise_complaints[\"Created Date\"] < end)].reset_index().drop(columns=[\"index\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311.to_pickle(path_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_complaints.to_pickle(\"data/311_Noise_Complaints_withNTA.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_complaints = pd.read_pickle(path_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_complaints = pd.read_pickle(\"data/311_Noise_Complaints_withNTA.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_complaints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add NTA information to complaints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Neighborhood Tabulation Areas (NTAs) were created to project populations at a small area level, from 2000 to 2030 for PlaNYC, the long-term sustainability plan for New York City. Since population size affects the error associated with population projections, these geographic units needed to have a minimum population, which we determined to be 15,000.\" [[source]](https://www1.nyc.gov/site/planning/data-maps/open-data/dwn-nynta.page)\n",
    "We have access to the population sizes of the NTAs, and one of our goals was to rank different neighborhoods based on criteria we can extract from the non-emergency 311 requests. Therefore we chose NTAs as the entities we would like to compare, because population sizes must be taken into account when inferring the state of a neighborhood from complaint frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:.2f}% of complaints don't have a location\".format(100 * (noise_complaints.Latitude.isnull().sum() / noise_complaints.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less than 2% of noise complaints in the sub-sampled data don't have a location, this is not much and does not need to be addressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load GeoJSON file containing NTAs\n",
    "with open(path_nta) as f:\n",
    "    js = json.load(f)\n",
    "\n",
    "def get_nta(row):\n",
    "    \"\"\"Retrieves the NTA of a given location in NYC.\n",
    "    Args:\n",
    "        row: 311 request with associated metadata (e.g. location)\n",
    "    Returns:\n",
    "        String: code of the NTA the location is part of, or NaN if the location is not within an NTA\n",
    "    \"\"\"\n",
    "    longitude = row[\"Longitude\"]\n",
    "    latitude = row[\"Latitude\"]\n",
    "    point = Point(longitude, latitude)\n",
    "    for feature in js[\"features\"]:\n",
    "        polygon = shape(feature[\"geometry\"])\n",
    "        if polygon.contains(point):\n",
    "            nta_code = feature[\"properties\"][\"ntacode\"]\n",
    "            return nta_code\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add NTA column\n",
    "noise_complaints[\"NTA\"] = np.nan\n",
    "noise_complaints.loc[noise_complaints.Latitude.notnull(), \"NTA\"] = noise_complaints[noise_complaints.Latitude.notnull()].apply(get_nta, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first look at the distribution of noise complaints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaint_count = noise_complaints.groupby(\"Complaint Type\") \\\n",
    "                                  .count()[\"Created Date\"] \\\n",
    "                                  .sort_values(ascending=False) \n",
    "      \n",
    "complaint_count = complaint_count[complaint_count != 0]\n",
    "ax = complaint_count.plot.barh(logx=True, figsize=(15, 10))\n",
    "ax.set_title(\"Complaint counts per noise complaint type\")\n",
    "ax.set_xlabel(\"Count (log scale)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse noise based on time of the day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **Seasonality** section, we saw that different types of complaints experience different seasonal patterns. Let's see what happens for noise complaints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns for date, weekday and hour of creation\n",
    "noise_complaints[\"Created Day\"] = noise_complaints[\"Created Date\"].dt.date\n",
    "noise_complaints[\"Created Hour\"] = noise_complaints[\"Created Date\"].dt.hour\n",
    "noise_complaints[\"Created Weekday\"] = noise_complaints[\"Created Date\"].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a period of one month for plotting\n",
    "month_start = datetime(2019, 9, 2)\n",
    "month_end = datetime(2019, 9, 30)\n",
    "\n",
    "# Filter complaints and group by the creation date, and distinguish noise complaint types\n",
    "daily_complaints = noise_complaints[(month_start <= noise_complaints[\"Created Date\"]) & (noise_complaints[\"Created Date\"] < month_end)] \\\n",
    "                            .groupby(['Created Day', 'Complaint Type'])['Created Day'] \\\n",
    "                            .count() \\\n",
    "                            .unstack('Complaint Type') \\\n",
    "                            .fillna(0)\n",
    "\n",
    "ax = daily_complaints.plot(kind='bar', stacked=True, figsize=(30,10), rot=45)\n",
    "ax.set_title(\"Noise complaints over the covered period\")\n",
    "ax.set_xlabel(\"Hour\")\n",
    "ax.set_ylabel(\"Count\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seasonality is obvious, there is a weekly pattern, with a peak in noise complaints over the weekend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_complaints = noise_complaints.groupby(['Created Hour', 'Complaint Type'])['Created Hour'].count().unstack('Complaint Type').fillna(0)\n",
    "ax = hourly_complaints.plot(kind='bar', stacked=True, figsize=(20,10), rot=0)\n",
    "ax.set_title(\"Aggregated noise complaints over the hours of a day\")\n",
    "ax.set_xlabel(\"Hour\")\n",
    "ax.set_ylabel(\"Count\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a daily pattern, with lots of noise complaints during the night. We might want to look at the pattern we get when aggregating based on the hour and the weekday, to see if there are differences and validate our current approach (disregard weekday). We expect to have a similar pattern every day, but with different numbers. The previous plots don't really show the relative evolution of the different noise complaints, let's try to normalize complaint counts to get a clearer picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize hourly noise complaint count\n",
    "hourly_complaints_normalized = hourly_complaints.div(hourly_complaints.sum(axis=1), axis=0)\n",
    "ax = hourly_complaints_normalized.plot(kind='bar', stacked=True, figsize=(20,10), rot=0)\n",
    "ax.set_title(\"Normalized noise complaint contributions over the hours of a day\")\n",
    "ax.set_xlabel(\"Hour\")\n",
    "ax.set_ylabel(\"Normalized contribution\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the noise complaint category, we can distinguish between night-time and day-time complaints as follows:\n",
    "- Day-time: Noise, Noise - Helicopter, Noise - House of Worship, Noise-Park, Noise - Vehical\n",
    "- Night-time: Noise - Residential, Noise - Street/Sidewalk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we assumed that the distribution of noise complaints does not depend on the weekday, let's check if that assumption is true, by plotting the complaint distribution for every weekday:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_complaints = noise_complaints.groupby(['Created Weekday', 'Created Hour'])[['Created Date']].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(x='Created Hour', y='Created Date', col='Created Weekday', data=hourly_complaints, kind='bar', col_wrap=2, sharey=False, height=4, aspect=2)\n",
    "g = g.set_ylabels(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shapes are not really that different, there are just peaks on Friday and Saturday night that must be taken into account and have an effect on the last 3 plots. The following question arises: is it just party-related / nightly complaints that are responsible for the weekend peaks? The following plot tries to provide an answer to that question, by looking at the distribution of noise complaints over the weekdays, for every hour of the day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(x='Created Weekday', y='Created Date', col='Created Hour', data=hourly_complaints, kind='bar', col_wrap=2, sharey=False, height=4, aspect=2)\n",
    "g = g.set_ylabels(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only are there more night-time noise-complaints on the weekend, there are also more day-time complaints, probably because people tend to be at home over the weekend, where they are more likely to be disturbed than at work. We notice that night-time complaints contribute a lot to the weekend-peak, this can be seen on the Friday evening/Saturday morning and Saturday evening/Sunday morning plots. \n",
    "\n",
    "This kind of behavior is something that should also be taken into account when **reasoning about other types of complaints**, i. e. when interpretating a number of complaints we should always think about factors like the weekday, the hour, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse noise based on location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to identify noisy NTAs. This is a first step to ranking the neighborhood tabulation areas, i. e. noise could be one of the criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load NTA population sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nta_pop_df = pd.read_csv(path_nta_population)\n",
    "nta_pop_df = nta_pop_df[nta_pop_df.Year == 2010].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A naive approach would only look at the total number of noise complaints of a NTA, but a more sensible way of measuring the exposure to noise would be to normalize the number of noise complaints, using the population size of the NTA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nta_complaints = noise_complaints.groupby(\"NTA\")[[\"Created Date\"]] \\\n",
    "                                 .count() \\\n",
    "                                 .rename(columns={\"Created Date\": \"Count\"})\n",
    "\n",
    "nta_complaints_extended = nta_complaints.merge(nta_pop_df.set_index(\"NTA Code\"), left_index=True, right_index=True)\n",
    "nta_complaints_extended[\"Normalized Count\"] = nta_complaints_extended[\"Count\"] / nta_complaints_extended[\"Population\"]\n",
    "nta_complaints_extended.loc[nta_complaints_extended.Population == 0, \"Normalized Count\"] = np.nan\n",
    "nta_complaints_extended = nta_complaints_extended.reset_index().set_index(\"NTA Name\").drop(columns=[\"level_0\"])\n",
    "nta_complaints_extended.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nta_complaints_extended[[\"Normalized Count\", \"Population\"]].sort_values(by=\"Normalized Count\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some NTAs correspond to parks or an airport, and have a much smaller population than the other NTAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nta_complaints_extended[nta_complaints_extended.Population < 10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compare the NTAs, sorted by the total number of noise complaints, and look at their normalized count rank (`NC Rank` in this dataframe), which is defined as the rank when comparing normalized complaint counts (1 corresponds to the highest normalized count):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nta_complaints_extended[\"NC Rank\"] = nta_complaints_extended[\"Normalized Count\"].rank(method=\"min\", ascending=False)\n",
    "nta_complaints_extended[[\"Count\", \"Population\", \"NC Rank\"]].sort_values(by=\"Count\", ascending=False).head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
